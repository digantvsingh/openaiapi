/*
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * OpenAPI spec version: 2.3.0
 * 
 *
 * NOTE: This class is auto generated by the swagger code generator program.
 * https://github.com/swagger-api/swagger-codegen.git
 * Do not edit the class manually.
 */

package com.openapi.client.model;

import java.util.Objects;
import java.util.Arrays;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonValue;
import com.openapi.client.model.ChatCompletionFunctions;
import com.openapi.client.model.ChatCompletionRequestMessage;
import com.openapi.client.model.ChatCompletionStreamOptions;
import com.openapi.client.model.ChatCompletionTool;
import com.openapi.client.model.ChatCompletionToolChoiceOption;
import com.openapi.client.model.CreateChatCompletionRequestAudio;
import com.openapi.client.model.CreateModelResponseProperties;
import com.openapi.client.model.ReasoningEffort;
import com.openapi.client.model.ResponseModalities;
import com.openapi.client.model.StopConfiguration;
import com.openapi.client.model.WebSearch;
import io.swagger.v3.oas.annotations.media.Schema;
import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
/**
 * CreateChatCompletionRequest
 */



public class CreateChatCompletionRequest extends CreateModelResponseProperties {
  @JsonProperty("messages")
  private List<ChatCompletionRequestMessage> messages = new ArrayList<ChatCompletionRequestMessage>();

  @JsonProperty("modalities")
  private ResponseModalities modalities = null;

  @JsonProperty("reasoning_effort")
  private ReasoningEffort reasoningEffort = null;

  @JsonProperty("max_completion_tokens")
  private Integer maxCompletionTokens = null;

  @JsonProperty("frequency_penalty")
  private BigDecimal frequencyPenalty = new BigDecimal(0);

  @JsonProperty("presence_penalty")
  private BigDecimal presencePenalty = new BigDecimal(0);

  @JsonProperty("web_search_options")
  private WebSearch webSearchOptions = null;

  @JsonProperty("top_logprobs")
  private Integer topLogprobs = null;

  @JsonProperty("response_format")
  private Object responseFormat = null;

  /**
   * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:   - If set to &#x27;auto&#x27;, and the Project is Scale tier enabled, the system     will utilize scale tier credits until they are exhausted.   - If set to &#x27;auto&#x27;, and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to &#x27;default&#x27;, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is &#x27;auto&#x27;.    When this parameter is set, the response body will include the &#x60;service_tier&#x60; utilized. 
   */
  public enum ServiceTierEnum {
    AUTO("auto"),
    DEFAULT("default");

    private String value;

    ServiceTierEnum(String value) {
      this.value = value;
    }
    @JsonValue
    public String getValue() {
      return value;
    }

    @Override
    public String toString() {
      return String.valueOf(value);
    }
    @JsonCreator
    public static ServiceTierEnum fromValue(String input) {
      for (ServiceTierEnum b : ServiceTierEnum.values()) {
        if (b.value.equals(input)) {
          return b;
        }
      }
      return null;
    }

  }  @JsonProperty("service_tier")
  private ServiceTierEnum serviceTier = null;

  @JsonProperty("audio")
  private CreateChatCompletionRequestAudio audio = null;

  @JsonProperty("store")
  private Boolean store = false;

  @JsonProperty("stream")
  private Boolean stream = false;

  @JsonProperty("stop")
  private StopConfiguration stop = null;

  @JsonProperty("logit_bias")
  private Map<String, Integer> logitBias = null;

  @JsonProperty("logprobs")
  private Boolean logprobs = false;

  @JsonProperty("max_tokens")
  private Integer maxTokens = null;

  @JsonProperty("n")
  private Integer n = 1;

  @JsonProperty("prediction")
  private Object prediction = null;

  @JsonProperty("seed")
  private Integer seed = null;

  @JsonProperty("stream_options")
  private ChatCompletionStreamOptions streamOptions = null;

  @JsonProperty("tools")
  private List<ChatCompletionTool> tools = null;

  @JsonProperty("tool_choice")
  private ChatCompletionToolChoiceOption toolChoice = null;

  @JsonProperty("parallel_tool_calls")
  private Boolean parallelToolCalls = null;

  @JsonProperty("function_call")
  private Object functionCall = null;

  @JsonProperty("functions")
  private List<ChatCompletionFunctions> functions = null;

  public CreateChatCompletionRequest messages(List<ChatCompletionRequestMessage> messages) {
    this.messages = messages;
    return this;
  }

  public CreateChatCompletionRequest addMessagesItem(ChatCompletionRequestMessage messagesItem) {
    this.messages.add(messagesItem);
    return this;
  }

   /**
   * A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). 
   * @return messages
  **/
  @Schema(required = true, description = "A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio). ")
  public List<ChatCompletionRequestMessage> getMessages() {
    return messages;
  }

  public void setMessages(List<ChatCompletionRequestMessage> messages) {
    this.messages = messages;
  }

  public CreateChatCompletionRequest modalities(ResponseModalities modalities) {
    this.modalities = modalities;
    return this;
  }

   /**
   * Get modalities
   * @return modalities
  **/
  @Schema(description = "")
  public ResponseModalities getModalities() {
    return modalities;
  }

  public void setModalities(ResponseModalities modalities) {
    this.modalities = modalities;
  }

  public CreateChatCompletionRequest reasoningEffort(ReasoningEffort reasoningEffort) {
    this.reasoningEffort = reasoningEffort;
    return this;
  }

   /**
   * Get reasoningEffort
   * @return reasoningEffort
  **/
  @Schema(description = "")
  public ReasoningEffort getReasoningEffort() {
    return reasoningEffort;
  }

  public void setReasoningEffort(ReasoningEffort reasoningEffort) {
    this.reasoningEffort = reasoningEffort;
  }

  public CreateChatCompletionRequest maxCompletionTokens(Integer maxCompletionTokens) {
    this.maxCompletionTokens = maxCompletionTokens;
    return this;
  }

   /**
   * An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). 
   * @return maxCompletionTokens
  **/
  @Schema(description = "An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning). ")
  public Integer getMaxCompletionTokens() {
    return maxCompletionTokens;
  }

  public void setMaxCompletionTokens(Integer maxCompletionTokens) {
    this.maxCompletionTokens = maxCompletionTokens;
  }

  public CreateChatCompletionRequest frequencyPenalty(BigDecimal frequencyPenalty) {
    this.frequencyPenalty = frequencyPenalty;
    return this;
  }

   /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#x27;s likelihood to repeat the same line verbatim. 
   * minimum: -2
   * maximum: 2
   * @return frequencyPenalty
  **/
  @Schema(description = "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. ")
  public BigDecimal getFrequencyPenalty() {
    return frequencyPenalty;
  }

  public void setFrequencyPenalty(BigDecimal frequencyPenalty) {
    this.frequencyPenalty = frequencyPenalty;
  }

  public CreateChatCompletionRequest presencePenalty(BigDecimal presencePenalty) {
    this.presencePenalty = presencePenalty;
    return this;
  }

   /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#x27;s likelihood to talk about new topics. 
   * minimum: -2
   * maximum: 2
   * @return presencePenalty
  **/
  @Schema(description = "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. ")
  public BigDecimal getPresencePenalty() {
    return presencePenalty;
  }

  public void setPresencePenalty(BigDecimal presencePenalty) {
    this.presencePenalty = presencePenalty;
  }

  public CreateChatCompletionRequest webSearchOptions(WebSearch webSearchOptions) {
    this.webSearchOptions = webSearchOptions;
    return this;
  }

   /**
   * Get webSearchOptions
   * @return webSearchOptions
  **/
  @Schema(description = "")
  public WebSearch getWebSearchOptions() {
    return webSearchOptions;
  }

  public void setWebSearchOptions(WebSearch webSearchOptions) {
    this.webSearchOptions = webSearchOptions;
  }

  public CreateChatCompletionRequest topLogprobs(Integer topLogprobs) {
    this.topLogprobs = topLogprobs;
    return this;
  }

   /**
   * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. &#x60;logprobs&#x60; must be set to &#x60;true&#x60; if this parameter is used. 
   * minimum: 0
   * maximum: 20
   * @return topLogprobs
  **/
  @Schema(description = "An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used. ")
  public Integer getTopLogprobs() {
    return topLogprobs;
  }

  public void setTopLogprobs(Integer topLogprobs) {
    this.topLogprobs = topLogprobs;
  }

  public CreateChatCompletionRequest responseFormat(Object responseFormat) {
    this.responseFormat = responseFormat;
    return this;
  }

   /**
   * An object specifying the format that the model must output.  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_schema\&quot;, \&quot;json_schema\&quot;: {...} }&#x60; enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to &#x60;{ \&quot;type\&quot;: \&quot;json_object\&quot; }&#x60; enables the older JSON mode, which ensures the message the model generates is valid JSON. Using &#x60;json_schema&#x60; is preferred for models that support it. 
   * @return responseFormat
  **/
  @Schema(description = "An object specifying the format that the model must output.  Setting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).  Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the message the model generates is valid JSON. Using `json_schema` is preferred for models that support it. ")
  public Object getResponseFormat() {
    return responseFormat;
  }

  public void setResponseFormat(Object responseFormat) {
    this.responseFormat = responseFormat;
  }

  public CreateChatCompletionRequest serviceTier(ServiceTierEnum serviceTier) {
    this.serviceTier = serviceTier;
    return this;
  }

   /**
   * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:   - If set to &#x27;auto&#x27;, and the Project is Scale tier enabled, the system     will utilize scale tier credits until they are exhausted.   - If set to &#x27;auto&#x27;, and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to &#x27;default&#x27;, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is &#x27;auto&#x27;.    When this parameter is set, the response body will include the &#x60;service_tier&#x60; utilized. 
   * @return serviceTier
  **/
  @Schema(description = "Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:   - If set to 'auto', and the Project is Scale tier enabled, the system     will utilize scale tier credits until they are exhausted.   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.   - When not set, the default behavior is 'auto'.    When this parameter is set, the response body will include the `service_tier` utilized. ")
  public ServiceTierEnum getServiceTier() {
    return serviceTier;
  }

  public void setServiceTier(ServiceTierEnum serviceTier) {
    this.serviceTier = serviceTier;
  }

  public CreateChatCompletionRequest audio(CreateChatCompletionRequestAudio audio) {
    this.audio = audio;
    return this;
  }

   /**
   * Get audio
   * @return audio
  **/
  @Schema(description = "")
  public CreateChatCompletionRequestAudio getAudio() {
    return audio;
  }

  public void setAudio(CreateChatCompletionRequestAudio audio) {
    this.audio = audio;
  }

  public CreateChatCompletionRequest store(Boolean store) {
    this.store = store;
    return this;
  }

   /**
   * Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. 
   * @return store
  **/
  @Schema(description = "Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products. ")
  public Boolean isStore() {
    return store;
  }

  public void setStore(Boolean store) {
    this.store = store;
  }

  public CreateChatCompletionRequest stream(Boolean stream) {
    this.stream = stream;
    return this;
  }

   /**
   * If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format). See the [Streaming section below](/docs/api-reference/chat/streaming) for more information, along with the [streaming responses](/docs/guides/streaming-responses) guide for more information on how to handle the streaming events. 
   * @return stream
  **/
  @Schema(description = "If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format). See the [Streaming section below](/docs/api-reference/chat/streaming) for more information, along with the [streaming responses](/docs/guides/streaming-responses) guide for more information on how to handle the streaming events. ")
  public Boolean isStream() {
    return stream;
  }

  public void setStream(Boolean stream) {
    this.stream = stream;
  }

  public CreateChatCompletionRequest stop(StopConfiguration stop) {
    this.stop = stop;
    return this;
  }

   /**
   * Get stop
   * @return stop
  **/
  @Schema(description = "")
  public StopConfiguration getStop() {
    return stop;
  }

  public void setStop(StopConfiguration stop) {
    this.stop = stop;
  }

  public CreateChatCompletionRequest logitBias(Map<String, Integer> logitBias) {
    this.logitBias = logitBias;
    return this;
  }

  public CreateChatCompletionRequest putLogitBiasItem(String key, Integer logitBiasItem) {
    if (this.logitBias == null) {
      this.logitBias = new HashMap<String, Integer>();
    }
    this.logitBias.put(key, logitBiasItem);
    return this;
  }

   /**
   * Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
   * @return logitBias
  **/
  @Schema(description = "Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. ")
  public Map<String, Integer> getLogitBias() {
    return logitBias;
  }

  public void setLogitBias(Map<String, Integer> logitBias) {
    this.logitBias = logitBias;
  }

  public CreateChatCompletionRequest logprobs(Boolean logprobs) {
    this.logprobs = logprobs;
    return this;
  }

   /**
   * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the &#x60;content&#x60; of &#x60;message&#x60;. 
   * @return logprobs
  **/
  @Schema(description = "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`. ")
  public Boolean isLogprobs() {
    return logprobs;
  }

  public void setLogprobs(Boolean logprobs) {
    this.logprobs = logprobs;
  }

  public CreateChatCompletionRequest maxTokens(Integer maxTokens) {
    this.maxTokens = maxTokens;
    return this;
  }

   /**
   * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of &#x60;max_completion_tokens&#x60;, and is not compatible with [o1 series models](/docs/guides/reasoning). 
   * @return maxTokens
  **/
  @Schema(description = "The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o1 series models](/docs/guides/reasoning). ")
  public Integer getMaxTokens() {
    return maxTokens;
  }

  public void setMaxTokens(Integer maxTokens) {
    this.maxTokens = maxTokens;
  }

  public CreateChatCompletionRequest n(Integer n) {
    this.n = n;
    return this;
  }

   /**
   * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep &#x60;n&#x60; as &#x60;1&#x60; to minimize costs.
   * minimum: 1
   * maximum: 128
   * @return n
  **/
  @Schema(example = "1", description = "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.")
  public Integer getN() {
    return n;
  }

  public void setN(Integer n) {
    this.n = n;
  }

  public CreateChatCompletionRequest prediction(Object prediction) {
    this.prediction = prediction;
    return this;
  }

   /**
   * Configuration for a [Predicted Output](/docs/guides/predicted-outputs), which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. 
   * @return prediction
  **/
  @Schema(description = "Configuration for a [Predicted Output](/docs/guides/predicted-outputs), which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content. ")
  public Object getPrediction() {
    return prediction;
  }

  public void setPrediction(Object prediction) {
    this.prediction = prediction;
  }

  public CreateChatCompletionRequest seed(Integer seed) {
    this.seed = seed;
    return this;
  }

   /**
   * This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. 
   * minimum: 9223372036854775616
   * maximum: -9223372036854775616
   * @return seed
  **/
  @Schema(description = "This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend. ")
  public Integer getSeed() {
    return seed;
  }

  public void setSeed(Integer seed) {
    this.seed = seed;
  }

  public CreateChatCompletionRequest streamOptions(ChatCompletionStreamOptions streamOptions) {
    this.streamOptions = streamOptions;
    return this;
  }

   /**
   * Get streamOptions
   * @return streamOptions
  **/
  @Schema(description = "")
  public ChatCompletionStreamOptions getStreamOptions() {
    return streamOptions;
  }

  public void setStreamOptions(ChatCompletionStreamOptions streamOptions) {
    this.streamOptions = streamOptions;
  }

  public CreateChatCompletionRequest tools(List<ChatCompletionTool> tools) {
    this.tools = tools;
    return this;
  }

  public CreateChatCompletionRequest addToolsItem(ChatCompletionTool toolsItem) {
    if (this.tools == null) {
      this.tools = new ArrayList<ChatCompletionTool>();
    }
    this.tools.add(toolsItem);
    return this;
  }

   /**
   * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. 
   * @return tools
  **/
  @Schema(description = "A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported. ")
  public List<ChatCompletionTool> getTools() {
    return tools;
  }

  public void setTools(List<ChatCompletionTool> tools) {
    this.tools = tools;
  }

  public CreateChatCompletionRequest toolChoice(ChatCompletionToolChoiceOption toolChoice) {
    this.toolChoice = toolChoice;
    return this;
  }

   /**
   * Get toolChoice
   * @return toolChoice
  **/
  @Schema(description = "")
  public ChatCompletionToolChoiceOption getToolChoice() {
    return toolChoice;
  }

  public void setToolChoice(ChatCompletionToolChoiceOption toolChoice) {
    this.toolChoice = toolChoice;
  }

  public CreateChatCompletionRequest parallelToolCalls(Boolean parallelToolCalls) {
    this.parallelToolCalls = parallelToolCalls;
    return this;
  }

   /**
   * Get parallelToolCalls
   * @return parallelToolCalls
  **/
  @Schema(description = "")
  public Boolean getParallelToolCalls() {
    return parallelToolCalls;
  }

  public void setParallelToolCalls(Boolean parallelToolCalls) {
    this.parallelToolCalls = parallelToolCalls;
  }

  public CreateChatCompletionRequest functionCall(Object functionCall) {
    this.functionCall = functionCall;
    return this;
  }

   /**
   * Deprecated in favor of &#x60;tool_choice&#x60;.  Controls which (if any) function is called by the model.  &#x60;none&#x60; means the model will not call a function and instead generates a message.  &#x60;auto&#x60; means the model can pick between generating a message or calling a function.  Specifying a particular function via &#x60;{\&quot;name\&quot;: \&quot;my_function\&quot;}&#x60; forces the model to call that function.  &#x60;none&#x60; is the default when no functions are present. &#x60;auto&#x60; is the default if functions are present. 
   * @return functionCall
  **/
  @Schema(description = "Deprecated in favor of `tool_choice`.  Controls which (if any) function is called by the model.  `none` means the model will not call a function and instead generates a message.  `auto` means the model can pick between generating a message or calling a function.  Specifying a particular function via `{\"name\": \"my_function\"}` forces the model to call that function.  `none` is the default when no functions are present. `auto` is the default if functions are present. ")
  public Object getFunctionCall() {
    return functionCall;
  }

  public void setFunctionCall(Object functionCall) {
    this.functionCall = functionCall;
  }

  public CreateChatCompletionRequest functions(List<ChatCompletionFunctions> functions) {
    this.functions = functions;
    return this;
  }

  public CreateChatCompletionRequest addFunctionsItem(ChatCompletionFunctions functionsItem) {
    if (this.functions == null) {
      this.functions = new ArrayList<ChatCompletionFunctions>();
    }
    this.functions.add(functionsItem);
    return this;
  }

   /**
   * Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. 
   * @return functions
  **/
  @Schema(description = "Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for. ")
  public List<ChatCompletionFunctions> getFunctions() {
    return functions;
  }

  public void setFunctions(List<ChatCompletionFunctions> functions) {
    this.functions = functions;
  }


  @Override
  public boolean equals(java.lang.Object o) {
    if (this == o) {
      return true;
    }
    if (o == null || getClass() != o.getClass()) {
      return false;
    }
    CreateChatCompletionRequest createChatCompletionRequest = (CreateChatCompletionRequest) o;
    return Objects.equals(this.messages, createChatCompletionRequest.messages) &&
        Objects.equals(this.modalities, createChatCompletionRequest.modalities) &&
        Objects.equals(this.reasoningEffort, createChatCompletionRequest.reasoningEffort) &&
        Objects.equals(this.maxCompletionTokens, createChatCompletionRequest.maxCompletionTokens) &&
        Objects.equals(this.frequencyPenalty, createChatCompletionRequest.frequencyPenalty) &&
        Objects.equals(this.presencePenalty, createChatCompletionRequest.presencePenalty) &&
        Objects.equals(this.webSearchOptions, createChatCompletionRequest.webSearchOptions) &&
        Objects.equals(this.topLogprobs, createChatCompletionRequest.topLogprobs) &&
        Objects.equals(this.responseFormat, createChatCompletionRequest.responseFormat) &&
        Objects.equals(this.serviceTier, createChatCompletionRequest.serviceTier) &&
        Objects.equals(this.audio, createChatCompletionRequest.audio) &&
        Objects.equals(this.store, createChatCompletionRequest.store) &&
        Objects.equals(this.stream, createChatCompletionRequest.stream) &&
        Objects.equals(this.stop, createChatCompletionRequest.stop) &&
        Objects.equals(this.logitBias, createChatCompletionRequest.logitBias) &&
        Objects.equals(this.logprobs, createChatCompletionRequest.logprobs) &&
        Objects.equals(this.maxTokens, createChatCompletionRequest.maxTokens) &&
        Objects.equals(this.n, createChatCompletionRequest.n) &&
        Objects.equals(this.prediction, createChatCompletionRequest.prediction) &&
        Objects.equals(this.seed, createChatCompletionRequest.seed) &&
        Objects.equals(this.streamOptions, createChatCompletionRequest.streamOptions) &&
        Objects.equals(this.tools, createChatCompletionRequest.tools) &&
        Objects.equals(this.toolChoice, createChatCompletionRequest.toolChoice) &&
        Objects.equals(this.parallelToolCalls, createChatCompletionRequest.parallelToolCalls) &&
        Objects.equals(this.functionCall, createChatCompletionRequest.functionCall) &&
        Objects.equals(this.functions, createChatCompletionRequest.functions) &&
        super.equals(o);
  }

  @Override
  public int hashCode() {
    return Objects.hash(messages, modalities, reasoningEffort, maxCompletionTokens, frequencyPenalty, presencePenalty, webSearchOptions, topLogprobs, responseFormat, serviceTier, audio, store, stream, stop, logitBias, logprobs, maxTokens, n, prediction, seed, streamOptions, tools, toolChoice, parallelToolCalls, functionCall, functions, super.hashCode());
  }


  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder();
    sb.append("class CreateChatCompletionRequest {\n");
    sb.append("    ").append(toIndentedString(super.toString())).append("\n");
    sb.append("    messages: ").append(toIndentedString(messages)).append("\n");
    sb.append("    modalities: ").append(toIndentedString(modalities)).append("\n");
    sb.append("    reasoningEffort: ").append(toIndentedString(reasoningEffort)).append("\n");
    sb.append("    maxCompletionTokens: ").append(toIndentedString(maxCompletionTokens)).append("\n");
    sb.append("    frequencyPenalty: ").append(toIndentedString(frequencyPenalty)).append("\n");
    sb.append("    presencePenalty: ").append(toIndentedString(presencePenalty)).append("\n");
    sb.append("    webSearchOptions: ").append(toIndentedString(webSearchOptions)).append("\n");
    sb.append("    topLogprobs: ").append(toIndentedString(topLogprobs)).append("\n");
    sb.append("    responseFormat: ").append(toIndentedString(responseFormat)).append("\n");
    sb.append("    serviceTier: ").append(toIndentedString(serviceTier)).append("\n");
    sb.append("    audio: ").append(toIndentedString(audio)).append("\n");
    sb.append("    store: ").append(toIndentedString(store)).append("\n");
    sb.append("    stream: ").append(toIndentedString(stream)).append("\n");
    sb.append("    stop: ").append(toIndentedString(stop)).append("\n");
    sb.append("    logitBias: ").append(toIndentedString(logitBias)).append("\n");
    sb.append("    logprobs: ").append(toIndentedString(logprobs)).append("\n");
    sb.append("    maxTokens: ").append(toIndentedString(maxTokens)).append("\n");
    sb.append("    n: ").append(toIndentedString(n)).append("\n");
    sb.append("    prediction: ").append(toIndentedString(prediction)).append("\n");
    sb.append("    seed: ").append(toIndentedString(seed)).append("\n");
    sb.append("    streamOptions: ").append(toIndentedString(streamOptions)).append("\n");
    sb.append("    tools: ").append(toIndentedString(tools)).append("\n");
    sb.append("    toolChoice: ").append(toIndentedString(toolChoice)).append("\n");
    sb.append("    parallelToolCalls: ").append(toIndentedString(parallelToolCalls)).append("\n");
    sb.append("    functionCall: ").append(toIndentedString(functionCall)).append("\n");
    sb.append("    functions: ").append(toIndentedString(functions)).append("\n");
    sb.append("}");
    return sb.toString();
  }

  /**
   * Convert the given object to string with each line indented by 4 spaces
   * (except the first line).
   */
  private String toIndentedString(java.lang.Object o) {
    if (o == null) {
      return "null";
    }
    return o.toString().replace("\n", "\n    ");
  }

}
